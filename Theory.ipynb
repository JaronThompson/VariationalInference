{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de42ede",
   "metadata": {},
   "source": [
    "# Variational inference using an invertible neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c84f29",
   "metadata": {},
   "source": [
    "Given a data set of input output pairs, $X = \\{(x_i, t_i)\\}_{i=1}^n$, a common goal of supervised machine learning is to train a model, $f$, to predict outcomes given new inputs. For regression tasks, we assume that the model predicts the expected value of outcomes and that observations of the outcome are noisy with zero mean Gaussian noise, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, \n",
    "\n",
    "$$\n",
    "t_i = f(x_i, z) + \\varepsilon\n",
    "$$\n",
    "where $z$ are the parameters of the model. In this case, we'll consider $z$ to be latent (unobserved) variables whose distribution must be inferred given the observed data, $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96503814",
   "metadata": {},
   "source": [
    "In other words, we want to determine the posterior distribution, $p(z | X)$. Variational inference provides a means of optimizing the parameters, $\\lambda$, of an approximate distribution $q(z | \\lambda)$ so that it matches the true posterior. The objective function that quantifies the difference between the approximate distribution and the true posterior is the Kullback-Leibler divergence. \n",
    "\n",
    "$$\n",
    "\\lambda^* = \\underset{\\lambda}{\\text{argmin}} \\; \\text{KL} \\; \\left[q(z | \\lambda) || p(z | X) \\right]  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d8c3f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{KL} \\; \\left[q(z | \\lambda) || p(z | X) \\right] = -\\int_z \\mathrm{ln} \\left( \\frac{p(z | X)}{q(z | \\lambda)} \\right) q(z | \\lambda) dz\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\int_z \\mathrm{ln}  p(z | X) \\; q(z | \\lambda) \\; dz \n",
    "+ \\int_z \\mathrm{ln} q(z | \\lambda) \\; q(z | \\lambda) \\; dz\n",
    "$$\n",
    "\n",
    "Using Bayes theorem, \n",
    "\n",
    "$$\n",
    "= -\\int_z \\mathrm{ln}  p(X | z) \\; q(z | \\lambda) \\; dz \n",
    "-\\int_z \\mathrm{ln}  p(z) \\; q(z | \\lambda) \\; dz \n",
    "+ \\underbrace{\\int_z \\mathrm{ln}  p(X) \\; q(z | \\lambda) \\; dz}_{\\text{const. w.r.t. z}}\n",
    "+ \\int_z \\mathrm{ln} q(z | \\lambda) \\; q(z | \\lambda) \\; dz\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b711a74",
   "metadata": {},
   "source": [
    "Ignoring terms that are constant w.r.t. $z$ (and $\\lambda$) gives the objective function\n",
    "\n",
    "$$\n",
    "J(\\lambda) = -\\int_z \\mathrm{ln}  p(X | z) \\; q(z | \\lambda) \\; dz \n",
    "-\\int_z \\mathrm{ln}  p(z) \\; q(z | \\lambda) \\; dz \n",
    "+ \\int_z \\mathrm{ln} q(z | \\lambda) \\; q(z | \\lambda) \\; dz\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e85b8",
   "metadata": {},
   "source": [
    "Generating samples from proposal distribution $q(z | \\lambda)$ is accomplished using a neural network with parameters $\\lambda$, which receives as input a random variable, $y$, whose distribution is easy to draw samples from. For example, $y \\sim p(y) = \\mathcal{U}[0, 1]$, so that $z = nn(y, \\lambda)$. The parameters of the neural network are determined by minimizing the objective function, $J(\\lambda)$, so that the neural network learns to translate samples from the base distribution into samples from the posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ab4f8",
   "metadata": {},
   "source": [
    "In order to evaluate the likelihood of a sample, $q(z|\\lambda)$, the neural network must be invertible so that the change of variables formula can be evaluated, \n",
    "\n",
    "$$\n",
    "q(z | \\lambda) = p(y) \\vert \\mathrm{det} \\; \\nabla_{z} nn^{-1}(z, \\lambda) \\vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249679c6",
   "metadata": {},
   "source": [
    "The objective can be evaluated using a Monte Carlo approach by drawing many candidate samples, $z_1, ..., z_m$, \n",
    "\n",
    "$$\n",
    "J(\\lambda) \\approx \\sum_{i=1}^{m} \\left( \\sum_{j=1}^{n} \\frac{1}{2 \\sigma^2} (t_j - f(x_j, z_i))^2 + \\frac{\\alpha}{2} z^T z + \\mathrm{ln} \\vert \\mathrm{det} \\; \\nabla_{z_i} nn^{-1}(z_i, \\lambda) \\vert  \\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
